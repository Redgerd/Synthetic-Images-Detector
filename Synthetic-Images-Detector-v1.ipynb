{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (100000, 28, 28, 1), Labels shape: (100000,)\n",
      "Testing data shape: (20000, 28, 28, 1), Labels shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2  # OpenCV for image processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(data_dir):\n",
    "    labels = []\n",
    "    \n",
    "    # Enumerate over the folders for real and fake images\n",
    "    for label, folder in enumerate(['real', 'fake']):\n",
    "        folder_path = os.path.join(data_dir, folder)\n",
    "        # Check if the folder exists\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Warning: Folder {folder_path} does not exist.\")\n",
    "            continue\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n",
    "            if image is None:\n",
    "                print(f\"Warning: Unable to load image {img_path}.\")\n",
    "                continue\n",
    "            image = cv2.resize(image, (28, 28))  # Resize to 28x28\n",
    "            image = image.astype('float32') / 255.0  # Normalize pixel values\n",
    "            images.append(image)\n",
    "            labels.append(label)  # 0 for real, 1 for fake\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Reshape to fit the input shape of the model\n",
    "    images = images.reshape(-1, 28, 28, 1)  # Add channel dimension\n",
    "    return images, labels\n",
    "\n",
    "# Load training and testing data from separate folders\n",
    "train_data_dir = 'train'  # Path to the training folder\n",
    "test_data_dir = 'test'     # Path to the testing folder\n",
    "\n",
    "# Load training data\n",
    "X_train, y_train = load_data(train_data_dir)\n",
    "# Load testing data\n",
    "X_test, y_test = load_data(test_data_dir)\n",
    "\n",
    "# Optionally split the training data if you want to use a validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, Labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, LeakyReLU, Dropout, Flatten, Dense\n",
    "\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Conv Block\n",
    "    model.add(Conv2D(32, 5, input_shape=(28, 28, 1)))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Second Conv Block\n",
    "    model.add(Conv2D(64, 5))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(LeakyReLU(0.2)) \n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Third Conv Block\n",
    "    model.add(Conv2D(128, 5))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Flatten and Dense layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the discriminator model\n",
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 42ms/step - accuracy: 0.6911 - loss: 0.7910 - val_accuracy: 0.7499 - val_loss: 0.6363\n",
      "Epoch 2/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 46ms/step - accuracy: 0.8165 - loss: 0.4297 - val_accuracy: 0.7661 - val_loss: 0.6285\n",
      "Epoch 3/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 46ms/step - accuracy: 0.8398 - loss: 0.3813 - val_accuracy: 0.8740 - val_loss: 0.3003\n",
      "Epoch 4/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 45ms/step - accuracy: 0.8570 - loss: 0.3379 - val_accuracy: 0.8751 - val_loss: 0.2926\n",
      "Epoch 5/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 44ms/step - accuracy: 0.8686 - loss: 0.3108 - val_accuracy: 0.8021 - val_loss: 0.4652\n",
      "Epoch 6/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 47ms/step - accuracy: 0.8761 - loss: 0.2954 - val_accuracy: 0.8800 - val_loss: 0.2889\n",
      "Epoch 7/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 44ms/step - accuracy: 0.8841 - loss: 0.2798 - val_accuracy: 0.7853 - val_loss: 0.5143\n",
      "Epoch 8/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 46ms/step - accuracy: 0.8865 - loss: 0.2733 - val_accuracy: 0.8393 - val_loss: 0.3917\n",
      "Epoch 9/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 47ms/step - accuracy: 0.8869 - loss: 0.2710 - val_accuracy: 0.8710 - val_loss: 0.2857\n",
      "Epoch 10/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 44ms/step - accuracy: 0.8918 - loss: 0.2593 - val_accuracy: 0.7124 - val_loss: 0.9348\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = discriminator.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.9061 - loss: 0.3036\n",
      "Test Accuracy: 71.24%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = discriminator.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
